{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNH2hs1GfsUX9yqawss5wMP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DeanAvram/Text-Processing/blob/main/Text_Processing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Loading & Basic Analysis"
      ],
      "metadata": {
        "id": "viEoTARsn4Q6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Tk2MjN8z3h8D"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import string\n",
        "import re\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "import spacy\n",
        "from spacy.tokenizer import Tokenizer\n",
        "from spacy.lang.en import English\n",
        "\n",
        "import time\n",
        "\n",
        "\n",
        "\n",
        "sms = pd.read_csv(\"/content/spam.csv\", encoding='latin-1')\n",
        "sms.dropna(how=\"any\", inplace=True, axis=1)\n",
        "sms.columns = ['label', 'message']\n",
        "sms.head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m nltk.downloader stopwords"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FsfYChhf2RhT",
        "outputId": "3504af8c-fb17-4778-f7be-7c12223fc801"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/lib/python3.10/runpy.py:126: RuntimeWarning: 'nltk.downloader' found in sys.modules after import of package 'nltk', but prior to execution of 'nltk.downloader'; this may result in unpredictable behaviour\n",
            "  warn(RuntimeWarning(msg))\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def clean(text):\n",
        "  sms = re.sub('[^a-zA-Z]', ' ', text) #Replacing all non-alphabetic characters with a space\n",
        "  sms = sms.lower() #converting to lowecase\n",
        "  #sms = sms.split()\n",
        "  #sms = ' '.join(sms)\n",
        "  return sms\n",
        "sms['message'] = sms['message'].apply(clean)"
      ],
      "metadata": {
        "id": "zFxL42Q9-kEg"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_stopwords(text):\n",
        "  stop_words = nltk.corpus.stopwords.words('english')\n",
        "  #str_text = [str(word) for word in text]\n",
        "  #filtered_text = list(filter(lambda word: word not in stop_words, str_text))\n",
        "  filtered_text = list(filter(lambda x: (not isinstance(x, spacy.tokens.token.Token) and x not in stop_words) or (isinstance(x, spacy.tokens.token.Token) and x.text not in stop_words), text))\n",
        "  #print('\\n')\n",
        "  return filtered_text"
      ],
      "metadata": {
        "id": "_332vPi8_kzo"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def most_frequent_words(df, col_name):\n",
        "  all_text = ' '.join(df[col_name].astype(str).tolist())\n",
        "  words = re.findall(r'\\b\\w+\\b', all_text.lower())\n",
        "  words_series = pd.Series(words)\n",
        "  word_counts = words_series.value_counts()\n",
        "  return word_counts"
      ],
      "metadata": {
        "id": "DVBIQCW9AlYY"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_statistics_on_df(df, col_name):\n",
        "  total_sms = df.shape[0]\n",
        "  ham_count = df['label'].value_counts()['ham']\n",
        "  spam_count = df['label'].value_counts()['spam']\n",
        "  num_words = df[col_name].apply(lambda x: len(x) if isinstance(x, (list, spacy.tokens.doc.Doc)) else len(x.split()))\n",
        "  frequent_words = most_frequent_words(df, col_name)\n",
        "  unique_words = frequent_words[frequent_words == 1].count()\n",
        "\n",
        "  print(f\"Total number of messages: {total_sms}\")\n",
        "  print(f\"Total number of HAM: {ham_count}\")\n",
        "  print(f\"Total number of SPAM: {spam_count}\")\n",
        "  print(f\"Average number of words per message: {np.mean(num_words)}\")\n",
        "  print(f\"Most frequent words:\\n{frequent_words.head(5)}\")\n",
        "  print(f\"Number of words that only appear once: {unique_words}\")"
      ],
      "metadata": {
        "id": "fJjc6cLStwGy"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "\n",
        "def print_statistics_on_text(text: list):\n",
        "  total_words = len(text)\n",
        "  word_counts = Counter(text)\n",
        "  most_frequent_words = word_counts.most_common(5)\n",
        "\n",
        "\n",
        "  print(f\"Total number of words: {total_words}\")\n",
        "  print(\"Most frequent words:\")\n",
        "  for word, count in most_frequent_words:\n",
        "    print(f\"{word}: {count}\")"
      ],
      "metadata": {
        "id": "gdYl8m7aDon-"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print_statistics_on_df(sms, 'message')"
      ],
      "metadata": {
        "id": "MpH0fIJN7k64"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text Processing"
      ],
      "metadata": {
        "id": "TnSpJNSen69b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenize"
      ],
      "metadata": {
        "id": "4nRSCNv-rV9u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### NLTK"
      ],
      "metadata": {
        "id": "8RnHRRFkafTw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "\n",
        "start_time = time.time()\n",
        "sms['nltk_tokenize_message'] = sms['message'].apply(lambda x: remove_stopwords(nltk.word_tokenize(x)))\n",
        "nltk_tokenize_time = time.time() - start_time\n",
        "sms"
      ],
      "metadata": {
        "id": "04YjsIVHn_gW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### SpaCy"
      ],
      "metadata": {
        "id": "0A8ReLDsahTe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = English()\n",
        "\n",
        "tokenizer = Tokenizer(nlp.vocab)\n",
        "start_time = time.time()\n",
        "sms['sapcy_tokenize_message'] = sms['message'].apply(lambda x: remove_stopwords(tokenizer(x)))\n",
        "spacy_tokenize_time = time.time() - start_time\n",
        "sms"
      ],
      "metadata": {
        "id": "q3BExveto1yV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tokenization Statistics"
      ],
      "metadata": {
        "id": "Th6Z1fd1a_0S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"nltk statistics\")\n",
        "print_statistics_on_df(sms, 'nltk_tokenize_message')\n",
        "print(f\"NLTK Processing Time: {nltk_tokenize_time}\")\n",
        "print(\"\\n\\nspacy statistics\")\n",
        "print_statistics_on_df(sms, 'sapcy_tokenize_message')\n",
        "print(f\"SpaCy Processing Time: {spacy_tokenize_time}\")"
      ],
      "metadata": {
        "id": "7_VEMdC2vtZx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tokenization Comparison"
      ],
      "metadata": {
        "id": "VvLwVYmK4ZW2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that the words statistics are almost the same. It says that the tokenization process tokenizes the text almost to the same tokens in both techniques.\n",
        "The NLTK tokenization splits the text into a list of tokens, in contrast to the SpaCy process that produces elements of Token objects.\n",
        "The processing time is almost the same."
      ],
      "metadata": {
        "id": "-_cjzx4l4mYH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Lemmatize"
      ],
      "metadata": {
        "id": "FFyyNeqTraOX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### NLTK"
      ],
      "metadata": {
        "id": "dcACtDOKajp2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('wordnet')\n",
        "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
        "start_time = time.time()\n",
        "sms['nltk_lemmatize_message'] = sms['nltk_tokenize_message'].apply(lambda x: [lemmatizer.lemmatize(word) for word in x])\n",
        "nltk_lemmatize_time = time.time() - start_time\n",
        "sms"
      ],
      "metadata": {
        "id": "AHBJ4xWYrwBi",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### SpaCy"
      ],
      "metadata": {
        "id": "8A0sf92Faosl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load('en_core_web_sm')\n",
        "start_time = time.time()\n",
        "sms['spacy_lemmatize_message'] = sms['sapcy_tokenize_message'].apply(lambda x: ' '.join(s.text for s in x))\n",
        "sms['spacy_lemmatize_message'] = sms['spacy_lemmatize_message'].apply(lambda x: ' '.join([token.lemma_ for token in nlp((x))]).split())\n",
        "spacy_lemmatize_time = time.time() - start_time\n",
        "sms"
      ],
      "metadata": {
        "id": "ChzsbbUUYX8J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Lemmatization Statistics"
      ],
      "metadata": {
        "id": "wijM-ClEa6rB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"nltk statistics\")\n",
        "print_statistics_on_df(sms, 'nltk_lemmatize_message')\n",
        "print(f\"NLTK Processing Time: {nltk_lemmatize_time}\")\n",
        "print(\"\\n\\nspacy statistics\")\n",
        "print_statistics_on_df(sms, 'spacy_lemmatize_message')\n",
        "print(f\"SpaCy Processing Time: {spacy_lemmatize_time}\")"
      ],
      "metadata": {
        "id": "2hq6xe2HYxlA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Lemmatization Comparison"
      ],
      "metadata": {
        "id": "fdSSR9G96I82"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "After the lemmatization, we can see that the word statistics is a bit different between the two methods. With NLTK, there are fewer tokens, which means that NLTK refers to more words as the same lemma. Also, the frequent words appear in different amounts in each technique. That means that the two techniques refer to tokens a bit differently.\n",
        "The processing time is much better with NLTK, as processing with SpaCy took significantly more time."
      ],
      "metadata": {
        "id": "OHUjAiFt6Mt5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stem"
      ],
      "metadata": {
        "id": "C9ElzJzKZWmu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### NLTK"
      ],
      "metadata": {
        "id": "fm1YVzwtaqcm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "stemmer = nltk.PorterStemmer()\n",
        "start_time = time.time()\n",
        "sms['nltk_stem_message'] = sms['nltk_tokenize_message'].apply(lambda x: [stemmer.stem(word) for word in x])\n",
        "nltk_stem_time = time.time() - start_time\n",
        "sms"
      ],
      "metadata": {
        "id": "DhjFbB7RZZfq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### SpaCy"
      ],
      "metadata": {
        "id": "rT1cjxWFar-q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#There is no Stemming library in SpaCy"
      ],
      "metadata": {
        "id": "puDurucFAgZ5"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Stem Statistics"
      ],
      "metadata": {
        "id": "7VuyX_h-axp1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"nltk statistics\")\n",
        "print_statistics_on_df(sms, 'nltk_stem_message')\n",
        "print(f\"NLTK Processing Time: {nltk_stem_time}\")\n",
        "#print(\"\\n\\nspacy statistics\")\n",
        "#print_statistics_on_df(sms, 'spacy_stem_message')"
      ],
      "metadata": {
        "id": "yG9yI2z5aZud"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Stemming Comparison"
      ],
      "metadata": {
        "id": "bu_iRpEO7dAV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There is only one way to stem the text. Only with NLTK.\n",
        "We can see that stemming refers to a token different from lemmatization.\n",
        "We can see that in the most frequent words, the order of them and the number of their appearance is different after the stemming compared to after lemmatization."
      ],
      "metadata": {
        "id": "5NHb7oZD7gmA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Web Scraping"
      ],
      "metadata": {
        "id": "AGDi9rB0zI0c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "url = 'https://en.wikipedia.org/wiki/Neuro-linguistic_programming'\n",
        "\n",
        "response = requests.get(url)\n",
        "soup_text = ''\n",
        "if response.status_code == 200:\n",
        "    # Parse the content of the request with BeautifulSoup\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "    # Extract all text from paragraph tags\n",
        "    paragraphs = soup.find_all('p')\n",
        "    for para in paragraphs:\n",
        "        soup_text += para.get_text()\n",
        "        #print(para.get_text())\n",
        "else:\n",
        "    print(f\"Failed to retrieve the webpage. Status code: {response.status_code}\")\n",
        "\n",
        "soup_text"
      ],
      "metadata": {
        "id": "0EnZWrqP6zDu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenize"
      ],
      "metadata": {
        "id": "l1ZjDm5iCT0k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "\n",
        "tokens = nltk.word_tokenize(soup_text)\n",
        "stopwords = nltk.corpus.stopwords.words('english')\n",
        "filtered_tokens = [token.lower() for token in tokens if token.lower() not in stopwords and token.isalpha()]\n",
        "filtered_tokens"
      ],
      "metadata": {
        "id": "UwMp6SZqCTHW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Lemmatize"
      ],
      "metadata": {
        "id": "bYc2J388DDWY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('wordnet')\n",
        "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
        "lemmatized_tokens = [lemmatizer.lemmatize(token) for token in filtered_tokens]\n",
        "lemmatized_tokens"
      ],
      "metadata": {
        "id": "bE6KcUG5DMVV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stem"
      ],
      "metadata": {
        "id": "6m9wLh2_FpwN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stemmer = nltk.PorterStemmer()\n",
        "stemmed_tokens = [stemmer.stem(token) for token in filtered_tokens]\n",
        "stemmed_tokens"
      ],
      "metadata": {
        "id": "lyXL7M3qFrtn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Statistics before text processing\\n')\n",
        "print_statistics_on_text(soup_text.split())\n",
        "print('\\nStatistics after text processing')\n",
        "print('\\nStatistics after tokenize\\n')\n",
        "print_statistics_on_text(filtered_tokens)\n",
        "print('\\nStatistics after lemmatize\\n')\n",
        "print_statistics_on_text(lemmatized_tokens)\n",
        "print('\\nStatistics after stem\\n')\n",
        "print_statistics_on_text(stemmed_tokens)"
      ],
      "metadata": {
        "id": "mvMLZ4QtEGQY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# WhatsApp Analysis"
      ],
      "metadata": {
        "id": "QJfQQqkAzMLH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Read and arrange WhatsApp txt file"
      ],
      "metadata": {
        "id": "Ox3Qsnb806AB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "with open(\"/content/_chat.txt\", encoding=\"utf-8\") as f:\n",
        "  lines = f.readlines()\n",
        "\n",
        "str_text = \"\"\n",
        "for line in lines:\n",
        "  # Extract the text after the name and colon\n",
        "  text = line[23:] #removing datetime\n",
        "  text = re.sub(r'^.*?:', '', text)\n",
        "  if re.search(r'[a-zA-Z]', text):\n",
        "    continue\n",
        "  text = str(text).strip()\n",
        "  str_text += text\n",
        "\n",
        "print(str_text)\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "NXbi10je0Gaa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenize"
      ],
      "metadata": {
        "id": "OPFeGFy71Mpg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from spacy.lang.he import Hebrew\n",
        "nlp = Hebrew()\n",
        "tokenizer = Tokenizer(nlp.vocab)\n",
        "tokens = tokenizer(str_text)\n",
        "print(list(tokens))"
      ],
      "metadata": {
        "id": "jOirDHON1Osh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Lemmatize"
      ],
      "metadata": {
        "id": "9HJAJ8fr4lsJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('wordnet')\n",
        "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
        "lemmatized_tokens = [lemmatizer.lemmatize(token.text) for token in tokens]\n",
        "print(lemmatized_tokens)"
      ],
      "metadata": {
        "id": "oVgMVBY04mhY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stem"
      ],
      "metadata": {
        "id": "2Iu_9nN959og"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "ps = PorterStemmer()\n",
        "stemmed_tokens = [ps.stem(token.text) for token in tokens]\n",
        "print(stemmed_tokens)"
      ],
      "metadata": {
        "id": "W5nW39BR59Bt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Statistics before text processing\\n')\n",
        "print_statistics_on_text(str_text.split())\n",
        "print('\\nStatistics after text processing')\n",
        "print('\\nStatistics after tokenize\\n')\n",
        "print_statistics_on_text(tokens)\n",
        "print('\\nStatistics after lemmatize\\n')\n",
        "print_statistics_on_text(lemmatized_tokens)\n",
        "print('\\nStatistics after stem\\n')\n",
        "print_statistics_on_text(stemmed_tokens)"
      ],
      "metadata": {
        "id": "8i6ey5nOH8zY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}